{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.feather as feather\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pathlib, os, pickle, time, base64, uuid\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "def load_pickle(thisPickle):\n",
    "    with open(str(thisPickle), 'rb') as handle:\n",
    "        batch_features = pickle.load(handle)\n",
    "    return batch_features.reshape(-1, 4096)\n",
    "\n",
    "def load_pickle_filesList(thisPickle):\n",
    "    with open(str(thisPickle), 'rb') as handle:\n",
    "        batch_elems = pickle.load(handle)\n",
    "    return batch_elems\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import a set of 600k features, a set of 100k features and our most recent checkpoint - a clustered set of 600,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickleFolder = pathlib.Path(\"faiss-test/600k\")\n",
    "feat_pickles = pickleFolder.rglob('features*.pickle')\n",
    "list_of_pickles =  [str(p) for p in feat_pickles]\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    features_list = list(tqdm(executor.map(load_pickle, list_of_pickles), total=len(list_of_pickles), desc=\"Loading Pickles\"))\n",
    "\n",
    "print(f\"concatenating features\")\n",
    "six_k_features = np.concatenate(features_list, axis=0)\n",
    "\n",
    "print(\"Reading FileNames\")\n",
    "feat_pickles = pickleFolder.rglob('filenames*.pickle')\n",
    "list_of_pickles =  [str(p) for p in feat_pickles]\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    six_k_filesList = list(itertools.chain.from_iterable(list(executor.map(load_pickle_filesList, list_of_pickles))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pickleFolder = pathlib.Path(\"faiss-test/100k\")\n",
    "feat_pickles = pickleFolder.rglob('features*.pickle')\n",
    "list_of_pickles =  [str(p) for p in feat_pickles]\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    features_list = list(tqdm(executor.map(load_pickle, list_of_pickles), total=len(list_of_pickles), desc=\"Loading Pickles\"))\n",
    "\n",
    "print(f\"concatenating features 2\")\n",
    "one_k_features = np.concatenate(features_list, axis=0)\n",
    "\n",
    "\n",
    "print(\"Reading FileNames 2\")\n",
    "feat_pickles = pickleFolder.rglob('filenames*.pickle')\n",
    "list_of_pickles =  [str(p) for p in feat_pickles]\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    one_k_filesList = list(itertools.chain.from_iterable(list(executor.map(load_pickle_filesList, list_of_pickles))))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass this set of features into our clusterer API we get the below graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "display(Image(filename='faiss/clustering-example.png'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll implement a FAISS database of the first 600,000 and then use this to predict the destination of the additional 100,000 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import faiss , sys\n",
    "import gc\n",
    "\n",
    "#change the individual rows into numpy arrays of type float32\n",
    "db_vectors = [np.array(x, dtype=\"float32\") for x in six_k_features ]\n",
    "\n",
    "#change it from a list of arrays to an array of arrays.\n",
    "db_vectors = np.array(db_vectors)\n",
    "\n",
    "dimension = len(db_vectors[0])    # dimensions of each vector                         \n",
    "n = len(db_vectors)    # number of vectors  \n",
    "\n",
    "print(dimension,n)\n",
    "\n",
    "nlist = int(9)  # number of clusters (see above image!)\n",
    "quantiser = faiss.IndexFlatL2(dimension)  \n",
    "index = faiss.IndexIVFFlat(quantiser, dimension, nlist,   faiss.METRIC_L2)\n",
    "\n",
    "index.train(db_vectors)\n",
    "index.add(db_vectors)\n",
    "\n",
    "del db_vectors, six_k_features\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our index now, so we can search against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the individual rows into numpy arrays of type float32\n",
    "search_vectors = [np.array(x, dtype=\"float32\") for x in one_k_features[1:10000] ]\n",
    "\n",
    "#change it from a list of arrays to an array of arrays.\n",
    "search_vectors = np.array(search_vectors)\n",
    "\n",
    "distances, indices = index.search(x=search_vectors,k=1)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we call the clustered results the \"truth\" then we can measure the success of FAISS below - ignoring the \"distances\" for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"faiss-test/groupPickle.pickle\",\"rb\") as pickleFile:\n",
    "        groups = pickle.load(pickleFile)\n",
    "\n",
    "# this is one group per cluster with filenames as the value of the sub-list. we want this the other way round, to lookup target group from filename:\n",
    "encodings = {file: group for group in groups for file in groups[group]}\n",
    "#encodings is now a dictionary with the key as the filename and the value as the group as predicted by clustering.\n",
    "successes = 0 \n",
    "fails = 0\n",
    "notfound = 0\n",
    "failDetails = []\n",
    "\n",
    "for i, idx in enumerate(indices.tolist()):\n",
    "        try:\n",
    "                matched_filename = six_k_filesList[idx[0]]\n",
    "                predicted_group = encodings[matched_filename]\n",
    "                actual_group = encodings[one_k_filesList[i]]\n",
    "                if predicted_group == actual_group:\n",
    "                        successes += 1\n",
    "                else:\n",
    "                        fails += 1\n",
    "                        failDetails.append({\"filename\":one_k_filesList[i],\"matched filename\":matched_filename,\"predicted group\":predicted_group,\n",
    "                                        \"actual group\":actual_group,\"distance\":distances[i]})\n",
    "        except:\n",
    "                notfound +=1\n",
    "\n",
    "print(f\"{fails+successes+notfound} matches checkd, {successes *100 / (fails+successes+notfound):.1f} % matches successful\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two we see that the \"correct\" group was predicted 37% of the time\n",
    "Getting the groups for all 700,000 took 111 seconds, while predicting the additional 100,000 groups took 50 minutes.\n",
    "\n",
    "FAISS is therefor a poor solution for this, and only really feasible when we want to classify a very small number of new images that are being added to a very large existing dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS should be good at this. What went wrong?\n",
    "Let's visualise some of the failed matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import load_img \n",
    "\n",
    "fail_examples = random.sample(failDetails,k=8)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (50,50))\n",
    "\n",
    "for index, fail in enumerate(fail_examples):\n",
    "        plt.subplot(10,2,(index*2)+1)\n",
    "        img = load_img(str(fail[\"filename\"]).replace(\"D:\\\\Allotment Timelapse\\\\\", \"Y:\\\\Allotment Timelapse\\\\GoPro TimeLapse\\\\\"))\n",
    "        img = np.array(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"search\")\n",
    "\n",
    "        plt.subplot(10,2,(index*2)+2)\n",
    "        img = load_img(str(fail[\"matched filename\"]).replace(\"D:\\\\Allotment Timelapse\\\\\", \"Y:\\\\Allotment Timelapse\\\\GoPro TimeLapse\\\\\"))\n",
    "        img = np.array(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"found\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timelapse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
